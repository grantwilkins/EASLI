environment: line 5: __conda_exe: command not found
environment: line 5: __conda_exe: command not found
[2024-06-19 16:31:07,764] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/ac.gwilkins/.conda/envs/mii/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible
[2024-06-19 16:31:38,064] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2024-06-19 16:31:38,065] [INFO] [runner.py:568:main] cmd = /home/ac.gwilkins/.conda/envs/mii/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None mii-inference.py --out_dir ./llama2-70b/2024-06-19/16-29-58 --hf_name /lcrc/project/ECP-EZ/ac.gwilkins/models--meta-llama--Llama-2-70b-chat-hf/snapshots/e9149a12809580e8602995856f8098ce973d1080 --dataset alpaca
[2024-06-19 16:32:50,871] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible
/home/ac.gwilkins/.conda/envs/mii/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[2024-06-19 16:33:18,333] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2024-06-19 16:33:18,333] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2024-06-19 16:33:18,333] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2024-06-19 16:33:18,333] [INFO] [launch.py:164:main] dist_world_size=2
[2024-06-19 16:33:18,333] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2024-06-19 16:33:18,336] [INFO] [launch.py:256:main] process 1657082 spawned with command: ['/home/ac.gwilkins/.conda/envs/mii/bin/python', '-u', 'mii-inference.py', '--local_rank=0', '--out_dir', './llama2-70b/2024-06-19/16-29-58', '--hf_name', '/lcrc/project/ECP-EZ/ac.gwilkins/models--meta-llama--Llama-2-70b-chat-hf/snapshots/e9149a12809580e8602995856f8098ce973d1080', '--dataset', 'alpaca']
[2024-06-19 16:33:18,337] [INFO] [launch.py:256:main] process 1657083 spawned with command: ['/home/ac.gwilkins/.conda/envs/mii/bin/python', '-u', 'mii-inference.py', '--local_rank=1', '--out_dir', './llama2-70b/2024-06-19/16-29-58', '--hf_name', '/lcrc/project/ECP-EZ/ac.gwilkins/models--meta-llama--Llama-2-70b-chat-hf/snapshots/e9149a12809580e8602995856f8098ce973d1080', '--dataset', 'alpaca']
/home/ac.gwilkins/.conda/envs/mii/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/home/ac.gwilkins/.conda/envs/mii/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[2024-06-19 16:35:09,371] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-19 16:35:09,371] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible
[2024-06-19 16:35:29,345] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-19 16:35:29,348] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-19 16:35:29,348] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[W socket.cpp:464] [c10d] The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use).
[W socket.cpp:464] [c10d] The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
[E socket.cpp:500] [c10d] The server socket has failed to listen on any local network address.
[2024-06-19 16:35:29,553] [INFO] [engine_v2.py:82:__init__] Building model...
Using /gpfs/fs1/home/ac.gwilkins/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Traceback (most recent call last):
  File "/gpfs/fs1/home/ac.gwilkins/EASLI/energy-measurements/mii-inference.py", line 107, in <module>
    pipe = mii.pipeline(hf_name)
  File "/home/ac.gwilkins/.conda/envs/mii/lib/python3.10/site-packages/mii/api.py", line 207, in pipeline
    inference_engine = load_model(model_config)
  File "/home/ac.gwilkins/.conda/envs/mii/lib/python3.10/site-packages/mii/modeling/models.py", line 14, in load_model
    init_distributed(model_config)
  File "/home/ac.gwilkins/.conda/envs/mii/lib/python3.10/site-packages/mii/utils.py", line 187, in init_distributed
    deepspeed.init_distributed(dist_backend="nccl", timeout=timedelta(seconds=1e9))
  File "/home/ac.gwilkins/.conda/envs/mii/lib/python3.10/site-packages/deepspeed/comm/comm.py", line 670, in init_distributed
    cdb = TorchBackend(dist_backend, timeout, init_method, rank, world_size)
  File "/home/ac.gwilkins/.conda/envs/mii/lib/python3.10/site-packages/deepspeed/comm/torch.py", line 112, in __init__
    self.init_process_group(backend, timeout, init_method, rank, world_size)
  File "/home/ac.gwilkins/.conda/envs/mii/lib/python3.10/site-packages/deepspeed/comm/torch.py", line 142, in init_process_group
    torch.distributed.init_process_group(backend,
  File "/home/ac.gwilkins/.conda/envs/mii/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
    return func(*args, **kwargs)
  File "/home/ac.gwilkins/.conda/envs/mii/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 89, in wrapper
    func_return = func(*args, **kwargs)
  File "/home/ac.gwilkins/.conda/envs/mii/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1305, in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
  File "/home/ac.gwilkins/.conda/envs/mii/lib/python3.10/site-packages/torch/distributed/rendezvous.py", line 246, in _env_rendezvous_handler
    store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout, use_libuv)
  File "/home/ac.gwilkins/.conda/envs/mii/lib/python3.10/site-packages/torch/distributed/rendezvous.py", line 174, in _create_c10d_store
    return TCPStore(
torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
Detected CUDA files, patching ldflags
Emitting ninja build file /gpfs/fs1/home/ac.gwilkins/.cache/torch_extensions/py310_cu121/inference_core_ops/build.ninja...
/home/ac.gwilkins/.conda/envs/mii/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module inference_core_ops...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[2024-06-19 16:35:33,475] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 1657082
[2024-06-19 16:35:33,476] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 1657083
[2024-06-19 16:35:33,694] [ERROR] [launch.py:325:sigkill_handler] ['/home/ac.gwilkins/.conda/envs/mii/bin/python', '-u', 'mii-inference.py', '--local_rank=1', '--out_dir', './llama2-70b/2024-06-19/16-29-58', '--hf_name', '/lcrc/project/ECP-EZ/ac.gwilkins/models--meta-llama--Llama-2-70b-chat-hf/snapshots/e9149a12809580e8602995856f8098ce973d1080', '--dataset', 'alpaca'] exits with return code = 1
[2024-06-19 16:36:50,829] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/ac.gwilkins/.conda/envs/mii/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible
[2024-06-19 16:37:15,096] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2024-06-19 16:37:15,096] [INFO] [runner.py:568:main] cmd = /home/ac.gwilkins/.conda/envs/mii/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None mii-inference.py --out_dir ./llama2-70b/2024-06-19/16-35-34 --hf_name /lcrc/project/ECP-EZ/ac.gwilkins/models--meta-llama--Llama-2-70b-chat-hf/snapshots/e9149a12809580e8602995856f8098ce973d1080 --dataset self-oss
[2024-06-19 16:38:34,320] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible
/home/ac.gwilkins/.conda/envs/mii/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[2024-06-19 16:38:59,443] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2024-06-19 16:38:59,443] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2024-06-19 16:38:59,443] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2024-06-19 16:38:59,443] [INFO] [launch.py:164:main] dist_world_size=2
[2024-06-19 16:38:59,443] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2024-06-19 16:38:59,445] [INFO] [launch.py:256:main] process 1662652 spawned with command: ['/home/ac.gwilkins/.conda/envs/mii/bin/python', '-u', 'mii-inference.py', '--local_rank=0', '--out_dir', './llama2-70b/2024-06-19/16-35-34', '--hf_name', '/lcrc/project/ECP-EZ/ac.gwilkins/models--meta-llama--Llama-2-70b-chat-hf/snapshots/e9149a12809580e8602995856f8098ce973d1080', '--dataset', 'self-oss']
[2024-06-19 16:38:59,446] [INFO] [launch.py:256:main] process 1662653 spawned with command: ['/home/ac.gwilkins/.conda/envs/mii/bin/python', '-u', 'mii-inference.py', '--local_rank=1', '--out_dir', './llama2-70b/2024-06-19/16-35-34', '--hf_name', '/lcrc/project/ECP-EZ/ac.gwilkins/models--meta-llama--Llama-2-70b-chat-hf/snapshots/e9149a12809580e8602995856f8098ce973d1080', '--dataset', 'self-oss']
/home/ac.gwilkins/.conda/envs/mii/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/home/ac.gwilkins/.conda/envs/mii/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[2024-06-19 16:41:07,680] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-19 16:41:07,681] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.

[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible
Downloading readme:   0%|          | 0.00/922 [00:00<?, ?B/s]Downloading readme: 100%|██████████| 922/922 [00:00<00:00, 5.66MB/s]
Downloading data:   0%|          | 0.00/90.1M [00:00<?, ?B/s]Downloading data:  12%|█▏        | 10.5M/90.1M [00:00<00:03, 24.0MB/s]Downloading data:  35%|███▍      | 31.5M/90.1M [00:00<00:01, 58.3MB/s]Downloading data:  58%|█████▊    | 52.4M/90.1M [00:00<00:00, 78.6MB/s]Downloading data:  81%|████████▏ | 73.4M/90.1M [00:00<00:00, 90.6MB/s]Downloading data: 100%|██████████| 90.1M/90.1M [00:01<00:00, 98.4MB/s]Downloading data: 100%|██████████| 90.1M/90.1M [00:01<00:00, 80.3MB/s]
Generating train split:   0%|          | 0/50661 [00:00<?, ? examples/s]Generating train split:  18%|█▊        | 9000/50661 [00:00<00:00, 81292.31 examples/s]Generating train split:  38%|███▊      | 19000/50661 [00:00<00:00, 82371.79 examples/s]Generating train split:  57%|█████▋    | 29000/50661 [00:00<00:00, 84457.22 examples/s]Generating train split:  75%|███████▌  | 38000/50661 [00:00<00:00, 85000.29 examples/s]Generating train split:  97%|█████████▋| 49000/50661 [00:00<00:00, 90455.14 examples/s]Generating train split: 100%|██████████| 50661/50661 [00:00<00:00, 86788.91 examples/s]
[2024-06-19 16:41:36,428] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-19 16:41:36,433] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-19 16:41:36,433] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[W socket.cpp:464] [c10d] The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use).
[W socket.cpp:464] [c10d] The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
[E socket.cpp:500] [c10d] The server socket has failed to listen on any local network address.
Traceback (most recent call last):
  File "/gpfs/fs1/home/ac.gwilkins/EASLI/energy-measurements/mii-inference.py", line 107, in <module>
    pipe = mii.pipeline(hf_name)
  File "/home/ac.gwilkins/.conda/envs/mii/lib/python3.10/site-packages/mii/api.py", line 207, in pipeline
    inference_engine = load_model(model_config)
  File "/home/ac.gwilkins/.conda/envs/mii/lib/python3.10/site-packages/mii/modeling/models.py", line 14, in load_model
    init_distributed(model_config)
  File "/home/ac.gwilkins/.conda/envs/mii/lib/python3.10/site-packages/mii/utils.py", line 187, in init_distributed
[2024-06-19 16:41:36,810] [INFO] [engine_v2.py:82:__init__] Building model...
    deepspeed.init_distributed(dist_backend="nccl", timeout=timedelta(seconds=1e9))
  File "/home/ac.gwilkins/.conda/envs/mii/lib/python3.10/site-packages/deepspeed/comm/comm.py", line 670, in init_distributed
    cdb = TorchBackend(dist_backend, timeout, init_method, rank, world_size)
  File "/home/ac.gwilkins/.conda/envs/mii/lib/python3.10/site-packages/deepspeed/comm/torch.py", line 112, in __init__
    self.init_process_group(backend, timeout, init_method, rank, world_size)
  File "/home/ac.gwilkins/.conda/envs/mii/lib/python3.10/site-packages/deepspeed/comm/torch.py", line 142, in init_process_group
    torch.distributed.init_process_group(backend,
  File "/home/ac.gwilkins/.conda/envs/mii/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
    return func(*args, **kwargs)
  File "/home/ac.gwilkins/.conda/envs/mii/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 89, in wrapper
    func_return = func(*args, **kwargs)
  File "/home/ac.gwilkins/.conda/envs/mii/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1305, in init_process_group
Using /gpfs/fs1/home/ac.gwilkins/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
    store, rank, world_size = next(rendezvous_iterator)
  File "/home/ac.gwilkins/.conda/envs/mii/lib/python3.10/site-packages/torch/distributed/rendezvous.py", line 246, in _env_rendezvous_handler
    store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout, use_libuv)
  File "/home/ac.gwilkins/.conda/envs/mii/lib/python3.10/site-packages/torch/distributed/rendezvous.py", line 174, in _create_c10d_store
    return TCPStore(
torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
[2024-06-19 16:41:38,615] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 1662652
[2024-06-19 16:41:38,616] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 1662653
[2024-06-19 16:41:38,834] [ERROR] [launch.py:325:sigkill_handler] ['/home/ac.gwilkins/.conda/envs/mii/bin/python', '-u', 'mii-inference.py', '--local_rank=1', '--out_dir', './llama2-70b/2024-06-19/16-35-34', '--hf_name', '/lcrc/project/ECP-EZ/ac.gwilkins/models--meta-llama--Llama-2-70b-chat-hf/snapshots/e9149a12809580e8602995856f8098ce973d1080', '--dataset', 'self-oss'] exits with return code = 1
[2024-06-19 16:42:48,960] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/ac.gwilkins/.conda/envs/mii/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible
[2024-06-19 16:43:13,004] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2024-06-19 16:43:13,004] [INFO] [runner.py:568:main] cmd = /home/ac.gwilkins/.conda/envs/mii/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None mii-inference.py --out_dir ./llama2-70b/2024-06-19/16-41-40 --hf_name /lcrc/project/ECP-EZ/ac.gwilkins/models--meta-llama--Llama-2-70b-chat-hf/snapshots/e9149a12809580e8602995856f8098ce973d1080 --dataset orca
[2024-06-19 16:44:33,420] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible
/home/ac.gwilkins/.conda/envs/mii/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[2024-06-19 16:45:00,546] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2024-06-19 16:45:00,546] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2024-06-19 16:45:00,546] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2024-06-19 16:45:00,546] [INFO] [launch.py:164:main] dist_world_size=2
[2024-06-19 16:45:00,546] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2024-06-19 16:45:00,548] [INFO] [launch.py:256:main] process 1668751 spawned with command: ['/home/ac.gwilkins/.conda/envs/mii/bin/python', '-u', 'mii-inference.py', '--local_rank=0', '--out_dir', './llama2-70b/2024-06-19/16-41-40', '--hf_name', '/lcrc/project/ECP-EZ/ac.gwilkins/models--meta-llama--Llama-2-70b-chat-hf/snapshots/e9149a12809580e8602995856f8098ce973d1080', '--dataset', 'orca']
[2024-06-19 16:45:00,549] [INFO] [launch.py:256:main] process 1668752 spawned with command: ['/home/ac.gwilkins/.conda/envs/mii/bin/python', '-u', 'mii-inference.py', '--local_rank=1', '--out_dir', './llama2-70b/2024-06-19/16-41-40', '--hf_name', '/lcrc/project/ECP-EZ/ac.gwilkins/models--meta-llama--Llama-2-70b-chat-hf/snapshots/e9149a12809580e8602995856f8098ce973d1080', '--dataset', 'orca']
/home/ac.gwilkins/.conda/envs/mii/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/home/ac.gwilkins/.conda/envs/mii/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[2024-06-19 16:47:10,246] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-19 16:47:10,246] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible
Downloading readme:   0%|          | 0.00/596 [00:00<?, ?B/s]Downloading readme: 100%|██████████| 596/596 [00:00<00:00, 3.69MB/s]
Downloading data:   0%|          | 0.00/125M [00:00<?, ?B/s]Downloading data:   8%|▊         | 10.5M/125M [00:00<00:04, 23.1MB/s]Downloading data:  25%|██▌       | 31.5M/125M [00:00<00:01, 55.8MB/s]Downloading data:  42%|████▏     | 52.4M/125M [00:00<00:00, 75.5MB/s]Downloading data:  59%|█████▉    | 73.4M/125M [00:01<00:00, 90.0MB/s]Downloading data:  76%|███████▌  | 94.4M/125M [00:01<00:00, 98.7MB/s]Downloading data:  93%|█████████▎| 115M/125M [00:01<00:00, 104MB/s]  Downloading data: 100%|██████████| 125M/125M [00:01<00:00, 86.4MB/s]
Generating train split:   0%|          | 0/54974 [00:00<?, ? examples/s]Generating train split: 100%|██████████| 54974/54974 [00:00<00:00, 66288.20 examples/s]Generating train split: 100%|██████████| 54974/54974 [00:00<00:00, 66192.82 examples/s]
[2024-06-19 16:47:42,562] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-19 16:47:42,563] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[W socket.cpp:464] [c10d] The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use).
[W socket.cpp:464] [c10d] The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
[E socket.cpp:500] [c10d] The server socket has failed to listen on any local network address.
[2024-06-19 16:47:42,567] [INFO] [comm.py:637:init_distributed] cdb=None
Traceback (most recent call last):
  File "/gpfs/fs1/home/ac.gwilkins/EASLI/energy-measurements/mii-inference.py", line 107, in <module>
    pipe = mii.pipeline(hf_name)
  File "/home/ac.gwilkins/.conda/envs/mii/lib/python3.10/site-packages/mii/api.py", line 207, in pipeline
    inference_engine = load_model(model_config)
  File "/home/ac.gwilkins/.conda/envs/mii/lib/python3.10/site-packages/mii/modeling/models.py", line 14, in load_model
    init_distributed(model_config)
  File "/home/ac.gwilkins/.conda/envs/mii/lib/python3.10/site-packages/mii/utils.py", line 187, in init_distributed
    deepspeed.init_distributed(dist_backend="nccl", timeout=timedelta(seconds=1e9))
  File "/home/ac.gwilkins/.conda/envs/mii/lib/python3.10/site-packages/deepspeed/comm/comm.py", line 670, in init_distributed
[2024-06-19 16:47:42,786] [INFO] [engine_v2.py:82:__init__] Building model...
    cdb = TorchBackend(dist_backend, timeout, init_method, rank, world_size)
  File "/home/ac.gwilkins/.conda/envs/mii/lib/python3.10/site-packages/deepspeed/comm/torch.py", line 112, in __init__
Using /gpfs/fs1/home/ac.gwilkins/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
    self.init_process_group(backend, timeout, init_method, rank, world_size)
  File "/home/ac.gwilkins/.conda/envs/mii/lib/python3.10/site-packages/deepspeed/comm/torch.py", line 142, in init_process_group
    torch.distributed.init_process_group(backend,
  File "/home/ac.gwilkins/.conda/envs/mii/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
    return func(*args, **kwargs)
  File "/home/ac.gwilkins/.conda/envs/mii/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 89, in wrapper
    func_return = func(*args, **kwargs)
  File "/home/ac.gwilkins/.conda/envs/mii/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1305, in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
  File "/home/ac.gwilkins/.conda/envs/mii/lib/python3.10/site-packages/torch/distributed/rendezvous.py", line 246, in _env_rendezvous_handler
    store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout, use_libuv)
  File "/home/ac.gwilkins/.conda/envs/mii/lib/python3.10/site-packages/torch/distributed/rendezvous.py", line 174, in _create_c10d_store
    return TCPStore(
torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
[2024-06-19 16:47:44,719] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 1668751
[2024-06-19 16:47:44,719] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 1668752
[2024-06-19 16:47:44,894] [ERROR] [launch.py:325:sigkill_handler] ['/home/ac.gwilkins/.conda/envs/mii/bin/python', '-u', 'mii-inference.py', '--local_rank=1', '--out_dir', './llama2-70b/2024-06-19/16-41-40', '--hf_name', '/lcrc/project/ECP-EZ/ac.gwilkins/models--meta-llama--Llama-2-70b-chat-hf/snapshots/e9149a12809580e8602995856f8098ce973d1080', '--dataset', 'orca'] exits with return code = 1
